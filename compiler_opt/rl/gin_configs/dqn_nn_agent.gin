import gin.tf.external_configurables
import compiler_opt.rl.gin_external_configurables
import tf_agents.agents.dqn.dqn_agent
import tf_agents.networks.q_network

train_eval.agent_name='dqn'
train_eval.num_policy_iterations=0
train_eval.num_iterations=20000
train_eval.batch_size=128
train_eval.train_sequence_length=2
train_eval.deploy_policy_name='saved_policy'

get_observation_processing_layer_creator.quantile_file_dir='compiler_opt/rl/vocab'
get_observation_processing_layer_creator.with_z_score_normalization = False

create_agent.policy_network = @q_network.QNetwork

QNetwork.preprocessing_combiner=@tf.keras.layers.Concatenate()
QNetwork.fc_layer_params=(100, 40)
QNetwork.dropout_layer_params=None
QNetwork.activation_fn=@tf.keras.activations.relu

tf.train.AdamOptimizer.learning_rate = 0.0003
tf.train.AdamOptimizer.epsilon = 0.0003125

DqnAgent.optimizer = @tf.train.AdamOptimizer()
DqnAgent.epsilon_greedy = 0.1
DqnAgent.target_update_tau = 0.8
DqnAgent.target_update_period= 128
DqnAgent.td_errors_loss_fn = None
DqnAgent.gamma = 0
DqnAgent.reward_scale_factor = 1.0
DqnAgent.gradient_clipping = None
DqnAgent.debug_summaries = True
DqnAgent.summarize_grads_and_vars = True
